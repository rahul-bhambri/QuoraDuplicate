{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some context to start with!**\n-------------------------------\n\n\nPageRank is an ingenious algorithm, developed by *Larry and Sergey*, arguably the biggest game-changer in the this world that we live in! Pagerank basically ranks the nodes in a graph structure, based on the linkages between them. An edge shared with an important node makes you important as well, a link with the spam node makes you spam too!\n\n## Why here? ##\n\nIn our context, every node is a question in our dataset and an edge represents a question pair. More often than not, an edge is shared by somehow related questions(topically), but may not be semantically equivalent -- This edge is however useful to visualize clusters of topics, and importance of certain nodes.\n\nThus, if a question is paired with a rather famous(higher pageranked) question, the question becomes relevant in itself.\n\nNote: The implementation of a Pagerank in this context was inspired by this [discussion][1] from Krzysztof Dziedzic\n\n\n  [1]: https://www.kaggle.com/c/quora-question-pairs/discussion/33664"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started, shall we!\n----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv').fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small function below computes a dictionary of questions, where each key-value pair is a question and its neighboring questions(in a list). This is necessary before we get along with calculating each question's pagerank! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a graph of Questions and their neighbors\n",
    "def generate_qid_graph_table(row):\n",
    "\n",
    "    hash_key1 = row[\"qid1\"]\n",
    "    hash_key2 = row[\"qid2\"]\n",
    "        \n",
    "    qid_graph.setdefault(hash_key1, []).append(hash_key2)\n",
    "    qid_graph.setdefault(hash_key2, []).append(hash_key1)\n",
    "\n",
    "qid_graph = {}\n",
    "df_train.apply(generate_qid_graph_table, axis = 1); #You should apply this on df_test too. Avoiding here on the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut to the chase! ##\n\nWithout getting into a lot of details, pagerank of a node is defined as the sum of a certain ratio of all its neighbors -- a complete dependence on adjacent vertices. The ratio is basically, the pagerank of the neighbor divided by the degree of the neighbor(edges incident on it). \n\nMathematically speaking,\nPR(n) = PR(n1)/num_neighbors(n1) + ... + PR(n_last)/num_neighbors(n_last)\n\nHowever, a damping factor is also induced in this formula, so as to account for how often the edge is to be taken(within the context of a random surfer on the web)\n\nThus, \n**PR(n) = (1-d)/N + d*(PR(n1)/num_neighbors(n1) + ... + PR(n_last)/num_neighbors(n_last))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank():\n",
    "\n",
    "    MAX_ITER = 20 #Let me know if you find an optimal iteration number!\n",
    "    d = 0.85\n",
    "    \n",
    "    #Initializing -- every node gets a uniform value!\n",
    "    pagerank_dict = {i:1/len(qid_graph) for i in qid_graph}\n",
    "    num_nodes = len(pagerank_dict)\n",
    "    \n",
    "    for iter in range(0, MAX_ITER):\n",
    "        \n",
    "        for node in qid_graph:    \n",
    "            local_pr = 0\n",
    "            \n",
    "            for neighbor in qid_graph[node]:\n",
    "                local_pr += pagerank_dict[neighbor]/len(qid_graph[neighbor])\n",
    "            \n",
    "            pagerank_dict[node] = (1-d)/num_nodes + d*local_pr\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "pagerank_dict = pagerank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially begin with a uniform pagerank value to all nodes, and with every iteration the pageranks begin to converge. You can also introduce a minimum difference between iterations to ensure convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the pageranks ##\n\nFinally, a getter function to concatenate the features with the rest of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagerank_value(row):\n",
    "    return pd.Series({\n",
    "        \"q1_pr\": pagerank_dict[row[\"qid1\"]],\n",
    "        \"q2_pr\": pagerank_dict[row[\"qid2\"]]\n",
    "    })\n",
    "\n",
    "pagerank_feats_train = df_train.apply(get_pagerank_value, axis = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result ##\n\nThese features gave me a slight 0.002 bump on a 100-nround xgboost, not a magic feature by any means ;)\n\nFork away :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}